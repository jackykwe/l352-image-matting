\documentclass{article}
\usepackage[margin=20mm]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{enumitem}

\def\vec#1{\underline{\mathbf{#1}}}
\def\vecbs#1{\underline{\boldsymbol{#1}}}
\def\mat#1{\underline{\underline{\mathbf{#1}}}}
\def\matbs#1{\underline{\underline{\boldsymbol{#1}}}}
\begin{document}
Firstly, some notes regarding notation I'm using. The paper mixes the use of both uppercase and lowercase letters for both matrices and vectors, so the capitalisation can't be used to differentiate clearly between vectors and matrices without relying on context (which is painfully ambiguous). Therefore for clarity I use the following notation, which mimics how I'd handwrite vectors and matrices:
\begin{itemize}
    \item Scalars are unbolded (e.g.\ $k$).
    \item Vectors are bolded and underlined once (e.g.\ $\vecbs\mu, \vec{I}$)
    \item Matrices are bolded and underlined twice (e.g.\ $\mat{L}$)
\end{itemize}

Two questions regarding paper \emph{A Closed-Form Solution to Natural Image Matting}.
\section{What exactly is the matting Laplacian $\mat{L}$?}
The paper seems to give two contradictory definitions of $\mat{L}$. I'll use notation for the coloured version (not the greyscale one) of the algorithm. First some auxiliary re-definitions so that we're on the same page:
\begin{itemize}
    \item We are analysing a $H\times W$ image. Pixels are indexed in some unspecified order from $0$ to $HW-1$. The paper uses $N=HW$.
    $$i,j,k \in \{0,\dots,HW-1\}$$
    All $i,j,k$s used in this PDF have the above domain.
    \item Though not mathematically defined by the paper, $w_i$ is the set of the (up to; pixels at edges and corners of original image may have fewer) 9 pixel indices in the neighbourhood of pixel $i$. Explicitly, I believe it is defined as
    \begin{align*}
        \forall i\in\{0,\dots,HW-1\}.\ w_i &= \{j \in \{0,\dots,HW-1\} \mid j\text{ is in the $3\times 3$ neighbourhood of }i\}\\
        &= \{i-W-1, i-W, i-W+1, i-1, i, i+1, i+W-1, i+W, i+W+1\} \\
        &\quad\ \cap \{0,\dots,HW-1\}
    \end{align*}
    The set intersection is just there to deal with the ``up to'' note I had above.
    \item I redefine the paper's equation (12) here to make my questions later clearer. $\forall i.\ |w_i|=3\times 3=9$ (window size).
    $$f(i,j) \overset{\text{def}}= \sum_{\substack{k\in\{0,\dots,HW-1\}:\\i\in w_k\wedge j\in w_k}} \left(\delta_{ij} - \frac19\left(1 + \left(\vec{I}_i - \vecbs\mu_k\right)^T \left(\matbs\Sigma_k + \frac\epsilon9 \mat{I}_{3\times 3}\right)^{-1} \left(\vec{I}_j - \vecbs\mu_k\right) \right)\right)\qquad\text{(identical to (12))}$$
    where
    \begin{itemize}
        \item[$\circ$] $\delta_{ij}=\begin{cases}1&\text{if $i=j$}\\0&\text{if $i\neq j$}\end{cases}$ is the Kronecker delta function,
        \item[$\circ$] $\vec{I}_i=\begin{bmatrix}I_i^R\\I_i^G\\I_i^B\end{bmatrix}$ is the $RGB$ triple of pixel $i$,
        \item[$\circ$] $\vecbs\mu_k=\frac19\sum_{l\in w_k}\vec{I}_l$ is the mean vector of the 9 $RGB$ triples in $w_k$,
        \item[$\circ$] $\matbs\Sigma_k = \frac19\sum_{l\in w_k} \left(\vec{I}_l-\vecbs\mu_k\right)\left(\vec{I}_l-\vecbs\mu_k\right)^T$ is the $3\times 3$ covariance matrix of the 9 $RGB$ triples in $w_k$ (I've written out the full derivation of equation (12) and this definition of $\matbs\Sigma_k$ is correct), and
        \item[$\circ$] $\mat{I}_{3\times3}$ is the $3\times 3$ identity matrix.
    \end{itemize}
\end{itemize}
For the $HW\times HW$ matrix $\mat{L} = \begin{bmatrix}
    L_{0,0} & L_{0,1} & \cdots & L_{0, HW-1}\\
    L_{1,0} & L_{1,1} & \cdots & L_{1, HW-1}\\
    \vdots & \vdots & \ddots & \vdots\\
    L_{HW-1,0} & L_{HW-1,1} & \cdots & L_{HW-1, HW-1}
\end{bmatrix}$, the paper provides two seemingly contradictory definitions:
\begin{enumerate}
    \item Right above equation (12), the paper states that
    $$L_{i,j} = f(i,j)$$
    \item In Section 5, the paper states that $\mat{L}$ can be re-written as $\mat{L}=\mat{D}-\mat{W}$ where
    \begin{align*}
        D_{i,j} &= \begin{cases}
            \sum_{j\in\{0,\dots,HW-1\}} W_{i,j},&\text{if $i=j$ (on diagonal)}\\
            0,&\text{otherwise}
        \end{cases}\\
        W_{i,j} &= f(i,j)
    \end{align*}
    The paper technically only states what non-diagonal entries for $W_{i,j}$ are; I assume $W_{i,i}$ are defined the same way (i.e.\ $W_{i,i}=f(i,i)$). From these we have
    \begin{align*}
        L_{i,j} &= D_{i,j} - W_{i,j}\\
        &= \begin{cases}
            \sum_{j\in\{0,\dots,HW-1\}\setminus\{i\}} f(i,j),&\text{if $i=j$ (on diagonal)}\\
            -f(i,j),&\text{otherwise}
        \end{cases}
    \end{align*}
    which matches other literature (e.g.\ Section 4.3.1 of \emph{Optimized Color Sampling for Robust Matting}, and what people typically refer to as a graph's ``Laplacian matrix'' as seen on \href{https://en.wikipedia.org/wiki/Laplacian_matrix#Laplacian_matrix}{Wikipedia}).
\end{enumerate}
So... which is correct? 1.\ and 2.\ are clearly different; did I misinterpret something?
\section{Regarding Theorem 4 and its corollary}
\begin{enumerate}
    \item There are two glaring typos that hinder understanding:
    \begin{enumerate}
        \item The theorem statement states
        $$\forall j>K.\ a_j^2 \leq \frac{\sum_{k=1}^K b_k^2}{\lambda_j} \leq \frac{\|b\|^2 \lambda_{\boxed{k}}}{\lambda_j}$$
        yet the conclusion states
        $$\forall j.\ a_j^2\lambda j \leq \sum_{k=1}^K b_k^2 \leq \|b\|^2 \lambda_{\boxed{K}}.$$

        I assume capital $K$ is the correct one, as the corollary uses capital $K$?
        \item The theorem statement states
        $$\forall j\boxed{>K}.\ a_j^2 \leq \frac{\sum_{k=1}^K b_k^2}{\lambda_j} \leq \frac{\|b\|^2 \lambda_{k}}{\lambda_j}$$
        yet the conclusion states
        $$\forall j.\ a_j^2\lambda j \leq \sum_{k=1}^K b_k^2 \leq \|b\|^2 \lambda_{K}.$$

        I assume the conclusion meant to say $\forall j>K$ as well?
    \end{enumerate}
    \item Why is the modulus of $b$ written using $\|\cdot\|$ instead of $|\cdot|$? Is $b$ actually a vector?
    \item Does the $a$ and $b$ in Theorem 4 have anything to do with the $\vec{a},b$ used in equation (9):
    $$\alpha_i \approx \vec{a}^T\vec{I}_i + b$$
    ? Everything is in lowercase unbolded text so it's ambiguous to me... I assume the $a,b$ here have nothing to do with equation 9?
    \item Finally, setting typographical errors aside, regarding the actual substance of Theorem 4's proof, I understand how equations (17) and (18) are derived: just substitute the definitions of $\vecbs\alpha=\sum_{k=1}^{HW}a_k\vec{v}_k$ and $\vecbs\beta=\sum_{k=1}^K b_k\vec{v}_k$ into $\vecbs\alpha^T \mat{L}\,\vecbs\alpha$ and $\vecbs\beta^T \mat{L}\,\vecbs\beta$, noting that the $\vec{v}_k$ are eigenvectors of $\mat{L}$. However, I don't understand how that leads to the conclusion of the proof?
    \item What is Theorem 4 even trying to say?
    \item What is the corollary even trying to say?
\end{enumerate}
\end{document}
